{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05bec0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\USER\\OneDrive\\Asztali gÃ©p\\Passion Projects\\CV\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import onnxruntime as ort\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab31df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMAL = 0\n",
    "FEATURES = 1 \n",
    "CANNY = 2\n",
    "FOURIER = 3\n",
    "GRAY = 4  \n",
    "FOURIER_HIGH = 5\n",
    "FOURIER_LOW = 6\n",
    "\n",
    "feature_params = dict(maxCorners=400, qualityLevel=0.1, minDistance=15, blockSize=9)\n",
    "\n",
    "camera_ind = 0\n",
    "source = cv2.VideoCapture(camera_ind)\n",
    "\n",
    "cv2.namedWindow(\"Camera\", cv2.WINDOW_NORMAL)\n",
    "alive = True\n",
    "image_filter = NORMAL\n",
    "\n",
    "while alive:\n",
    "    has_frame, frame = source.read()\n",
    "\n",
    "    if not has_frame:\n",
    "        break\n",
    "\n",
    "    if image_filter == NORMAL:\n",
    "        result = frame\n",
    "\n",
    "    if image_filter == CANNY:\n",
    "        result = cv2.Canny(frame, 80, 150)\n",
    "\n",
    "    if image_filter == FEATURES:\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        corners = cv2.goodFeaturesToTrack(frame_gray, **feature_params)\n",
    "        if corners is not None:\n",
    "            result = frame.copy()\n",
    "            for x, y in np.float32(corners).reshape(-1, 2):\n",
    "                result = cv2.circle(frame, (int(x),int(y)), 10, (255, 0, 255), 1)\n",
    "\n",
    "    if image_filter == FOURIER:\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        spectrum = np.fft.fft2(frame_gray)\n",
    "        centered_spectrum = np.fft.fftshift(spectrum)\n",
    "        fourier = np.log(np.abs(centered_spectrum))\n",
    "        fourier = cv2.normalize(\n",
    "        fourier,\n",
    "        None,\n",
    "        0,\n",
    "        255,\n",
    "        cv2.NORM_MINMAX\n",
    "        )\n",
    "\n",
    "        result = fourier.astype(np.uint8)\n",
    "\n",
    "    if image_filter == FOURIER_HIGH:\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        spectrum = np.fft.fft2(frame_gray)\n",
    "        centered_spectrum = np.fft.fftshift(spectrum)\n",
    "        start_row = (frame_gray.shape[0]- 55)//2\n",
    "        start_col = (frame_gray.shape[1]- 55)//2\n",
    "\n",
    "        centered_spectrum[start_row:start_row+55, start_col:start_col+55] = 0\n",
    "\n",
    "        spect = np.fft.ifftshift(centered_spectrum)\n",
    "        fourier = np.real(np.fft.ifft2(spect))\n",
    "\n",
    "        fourier = cv2.normalize(\n",
    "        fourier,\n",
    "        None,\n",
    "        0,\n",
    "        255,\n",
    "        cv2.NORM_MINMAX\n",
    "        )\n",
    "\n",
    "        result = fourier.astype(np.uint8)\n",
    "\n",
    "    if image_filter == FOURIER_LOW:\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        spectrum = np.fft.fft2(frame_gray)\n",
    "        centered_spectrum = np.fft.fftshift(spectrum)\n",
    "        start_row = (frame_gray.shape[0]- 20)//2\n",
    "        start_col = (frame_gray.shape[1]- 20)//2\n",
    "        zero = np.zeros(shape=(frame_gray.shape[0], frame_gray.shape[1]))\n",
    "\n",
    "        zero[start_row:start_row+20, start_col:start_col+20] = 1\n",
    "        centered_spectrum = centered_spectrum * zero\n",
    "\n",
    "        spect = np.fft.ifftshift(centered_spectrum)\n",
    "        fourier = np.real(np.fft.ifft2(spect))\n",
    "\n",
    "        fourier = cv2.normalize(\n",
    "        fourier,\n",
    "        None,\n",
    "        0,\n",
    "        255,\n",
    "        cv2.NORM_MINMAX\n",
    "        )\n",
    "\n",
    "        result = fourier.astype(np.uint8)\n",
    "\n",
    "    if image_filter == GRAY:\n",
    "        result = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    cv2.imshow(\"Camera\", result)\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        alive = False\n",
    "    if key == ord('n'):\n",
    "        image_filter = NORMAL\n",
    "    if key == ord('c'):\n",
    "        image_filter = CANNY\n",
    "    if key == ord('d'):\n",
    "        image_filter = FEATURES\n",
    "    if key == ord('f'):\n",
    "        image_filter = FOURIER\n",
    "    if key == ord('h'):\n",
    "        image_filter = FOURIER_HIGH\n",
    "    if key == ord('l'):\n",
    "        image_filter = FOURIER_LOW\n",
    "    if key == ord('g'):\n",
    "        image_filter = GRAY\n",
    "\n",
    "source.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0938a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "prev_time = time.time()\n",
    "\n",
    "source = cv2.VideoCapture(0)\n",
    "alive = True\n",
    "session = ort.InferenceSession(\"yolov11m-face-dynamic.onnx\")\n",
    "tracker = DeepSort(\n",
    "    max_age=50,\n",
    "    n_init=2,\n",
    "    nms_max_overlap=0.4,\n",
    ")\n",
    "\n",
    "input_name = session.get_inputs()[0].name\n",
    "input_shape = session.get_inputs()[0].shape\n",
    "deep_input = []\n",
    "\n",
    "\n",
    "def preprocess(frame):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = cv2.resize(frame, (128, 128))\n",
    "    frame = np.transpose(frame, (2, 0, 1))\n",
    "    frame = np.expand_dims(frame, axis=0)\n",
    "    frame = frame.astype(np.float32) / 255.0\n",
    "    return frame\n",
    "\n",
    "\n",
    "def yolo_nms(preds, scaleX, scaleY, conf_threshold=0.75, nms_threshold=0.4):\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    \n",
    "    for detection in preds:\n",
    "        if detection[4] > conf_threshold:\n",
    "            x_center, y_center, w, h, conf = detection\n",
    "            \n",
    "            x_min = int((x_center - w/2) * scaleX)\n",
    "            y_min = int((y_center - h/2) * scaleY)\n",
    "            width = int(w * scaleX)\n",
    "            height = int(h * scaleY)\n",
    "            \n",
    "            boxes.append([x_min, y_min, width, height])\n",
    "            confidences.append(float(conf))\n",
    "    \n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "    \n",
    "    output = []\n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            output.append(([x, y, w, h], confidences[i], 0))\n",
    "    \n",
    "    return output\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while alive:\n",
    "    ok, frame = source.read()\n",
    "    h = frame.shape[0]\n",
    "    w = frame.shape[1]\n",
    "    scaleX = w/128\n",
    "    scaleY = h/128\n",
    "    frame_count += 1\n",
    "\n",
    "    if not ok:\n",
    "        break\n",
    "\n",
    "\n",
    "    if (frame_count % 2) == 0:\n",
    "        blob = preprocess(frame)\n",
    "        detections = session.run(None, {input_name: blob})\n",
    "        preds = detections[0][0]\n",
    "        preds = preds.T\n",
    "        deep_input = yolo_nms(preds, scaleX, scaleY) \n",
    "    \n",
    "    tracks = tracker.update_tracks(deep_input, frame=frame)\n",
    "    \n",
    "    for track in tracks:\n",
    "\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        ltrb = track.to_ltrb()\n",
    "        x_min, y_min, x_max, y_max = map(int, ltrb)\n",
    "\n",
    "        face = frame[y_min-5:y_max+5, x_min-5:x_max+5, :]\n",
    "\n",
    "        if face.size == 0:\n",
    "            continue\n",
    "\n",
    "        result = DeepFace.analyze(\n",
    "            face,\n",
    "            actions=['age', 'gender', 'emotion', 'race'],\n",
    "            enforce_detection=False,\n",
    "            detector_backend='skip',\n",
    "            silent=True\n",
    "        )\n",
    "        \n",
    "        emotion = result[0]['dominant_emotion']\n",
    "        emotions = result[0]['emotion']\n",
    "        conf = emotions[emotion] / 100\n",
    "\n",
    "        #age = result[0]['age']\n",
    "        #gender = result[0]['dominant_gender']\n",
    "        #race = result[0]['dominant_race']\n",
    "\n",
    "        curr_time = time.time()\n",
    "        fps = 1 / ((curr_time - prev_time) + 0.1)\n",
    "        prev_time = curr_time\n",
    "    \n",
    "        frame = cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (255, 0, 0), 2)\n",
    "        frame = cv2.putText(frame, f\"ID: {track_id}; Emotion: {emotion} & Score: {conf:.2f}\", (x_min, y_min-5), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 0), 2)\n",
    "        #frame = cv2.putText(frame, f\"Age: {age:.2f} & Gender: {gender} & Race: {race}\", (x_min, y_min-20), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 0), 2)\n",
    "\n",
    "    frame =  cv2.putText(frame, f\"FPS: {fps:.1f}\", (10, 30), cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        alive = False\n",
    "    cv2.imshow('Face', frame)\n",
    "\n",
    "source.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d77127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
